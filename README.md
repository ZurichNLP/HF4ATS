# HF4ATS: Human Feedback for Automatic Text Simplification

This repository contains the code for the data processing, web application, model training, experiments, and analyses described in the paper ["Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities"]().

All code and data is intended for non-commercial research use only â€“ please refer to the accompanying license/copyright notice for details.

## HF4ATS Data

HF4ATS (**H**uman **F**eedback **For** **A**utomatic **T**ext **S**implification) is a German-language collection of news sentence simplifications and simplification preferences suitable for automatic simplification preference alignment and/or fine-tuning. It is composed of:
1. <u>HF4ATS-DPO</u> Automatic text simplification (ATS) preference pairs annotated by both text simplification experts and persons with cognitive impairments. This dataset is suitable for preference alignment.
2. <u>HF4ATS-SFT</u> Complex-simple manual text simplification pairs. This dataset is suitable for supervised fine-tuning.

HF4ATS-DPO data is available at [Zenodo].
HF4ATS-SFT data is available as `data/sft_<type>.jsonl`, where `<model>` can be `train`, `dev`, and `holdout`. This data is sourced from DEplain-APA data, available at [Zenodo](https://zenodo.org/records/8304430). 

## Usage

Requirements:

* Python == 3.9

```bash
pip install -r requirements.txt
```

### Preparing DEPlain-APA data for Supervised Fine-Tuning

Train, development, and test HF4ATS-SFT data is available already in `data/`. That said, it can be reproduced from the raw DEplain-APA data. 

To do so, first download the [DEPlain-APA](https://zenodo.org/records/8304430) data. Place `all.csv` inside `data/deplain_sentences/`.

Next, run the following script:

```bash
python pre_sft.py
```

At this point, the complex-simple SFT data is ready for use under `data/sft_train.jsonl`, `data/sft_dev.jsonl`, and `data/sft_holdout.jsonl`.

### Supervised Fine-Tuning

Our SFT model checkpoints are available at [swissubase]. 

Should you wish to perform SFT, you must first ensure you have access (if required) to the following four models on HuggingFace:

https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3
https://huggingface.co/DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1 
https://huggingface.co/LeoLM/leo-mistral-hessianai-7b-chat

Next, input your HuggingFace token in place of the text `<YOUR HUGGINGFACE TOKEN HERE>` inside `utils/base_dependencies.py`

Finally, run the following script:

```bash
python sft_wrapper.py "<model>" bs16ga"<ga>"dv"<gpu>"lr"<lr>"
```

`<model>` must be from the following set: [disco_llama8b, llama8b, leolm_mistral7b, mistral7b]

`<ga>`, `<gpu>`, and `<lr>` refer to gradient accumulation steps, GPU count, and learning rate respectively. 

The post-grid-search parameter set argument we used for SFT was bs16ga1dv1lr1e-4.

### Direct Preference Optimization

Our DPO model checkpoints are available at [swissubase].

Should you wish to perform DPO, you must first download the HF4ATS-DPO preference data at [Zenodo]. 

The HF4ATS-DPO Zenodo record contains all raw data obtained during annotation and final evaluation as well as several subset of this data. To run DPO, store these json files inside `data/preferences/`.

You must also have the necessary SFT checkpoints saved with the proper name and at the proper location. The rules are as follows:
1. Let `<model>` be a string from the following set: [disco_llama8b, llama8b, leolm_mistral7b, mistral7b] 
2. Let `####` inside `checkpoint####` refer to the number of training instances seen during SFT (e.g. 2800 for our disco_llama8b checkpoint).
2. Save the SFT checkpoint being post-trained at `outputs/models/bs16ga1dv1lr1e-4/sft_<model>/checkpoint####/` folder.

Next, run the following script to perform DPO:

```bash
python dpo_wrapper.py "bs16ga1dv1lr1e-4/sft_<model>_checkpoint####" <variant> <user_set> train

`<user_set>` can be either "ta" (target group annotations) or "ea" (expert group annotations)

`<variant>` can be as follows:

1. "all": all preferences (see Zenodo record for information about raw preference data deduplication and exclusion).
2. "equality": all preferences on ATS pairs which had equal levels of information, according to pair creators.
3. "model": all preferences on ATS pairs generated by the SFT checkpoint being post-trained (assumes this preference data is being used to post-train our specific SFT checkpoints).
4. "intraAA": all preferences indicated by the four (for target group) or two (expert group) annotators with the highest intra-annotation agreement.
5. "interAA": all preferences indicated by the four (for target group) or two (expert group) annotators with the highest inter-annotation agreement.
6. "groupX": uses "all" expert-group preferences when training and "intraAA" target-group preferences during evaluation (must set <user_set> to 'ea' when using this preference subset!)

### Automatic Evaluation Metrics for an SFT Checkpoint



## Copyright notice

The resulting dataset is released with the following copyright notice:

### German / Deutsch:

### English / Englisch:

## Citation

If you use the dataset, please cite the following paper:

```
@misc{ 
}
```




